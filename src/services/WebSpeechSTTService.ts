/**
 * WEB SPEECH STT SERVICE - RECONNAISSANCE VOCALE POUR CONVERSATIONS TH√âRAPEUTIQUES
 * Service de reconnaissance vocale optimis√© pour sessions th√©rapeutiques
 * Support fran√ßais avec d√©tection √©motionnelle basique
 * Date: 30/08/2025
 */

// Types pour reconnaissance vocale conversationnelle
export interface STTRequest {
  language: string;
  continuous: boolean;
  interimResults: boolean;
  maxAlternatives: number;
  expertId?: string;
  emotionalContext?: string;
}

export interface STTResponse {
  transcript: string;
  confidence: number;
  isFinal: boolean;
  alternatives?: {
    transcript: string;
    confidence: number;
  }[];
  detectedEmotion?: string;
  speechDuration?: number;
}

export interface STTSession {
  id: string;
  isActive: boolean;
  startTime: Date;
  config: STTRequest;
  onResult: (result: STTResponse) => void;
  onError: (error: string) => void;
  onEnd: () => void;
}

/**
 * SERVICE DE RECONNAISSANCE VOCALE TH√âRAPEUTIQUE
 * Utilise Web Speech API avec optimisations pour conversations th√©rapeutiques
 */
export class WebSpeechSTTService {
  private static instance: WebSpeechSTTService;
  private recognition: SpeechRecognition | null = null;
  private currentSession: STTSession | null = null;
  private isSupported: boolean = false;
  private sessionCounter: number = 0;

  // Configuration optimis√©e pour conversations th√©rapeutiques
  private readonly DEFAULT_CONFIG: STTRequest = {
    language: 'fr-FR',
    continuous: true,
    interimResults: true,
    maxAlternatives: 3
  };

  // Mots-cl√©s √©motionnels pour d√©tection basique
  private readonly EMOTIONAL_KEYWORDS = {
    'anxieux': ['anxieux', 'angoiss√©', 'stress√©', 'inquiet', 'panique', 'tendu'],
    'triste': ['triste', 'd√©prim√©', 'm√©lancolique', 'abattu', 'morose', 'd√©courag√©'],
    'col√®re': ['en col√®re', 'furieux', 'irrit√©', '√©nerv√©', 'frustr√©', 'agac√©'],
    'joie': ['heureux', 'content', 'joyeux', 'ravi', 'euphorique', 'enthousiaste'],
    'peur': ['peur', 'effray√©', 'terroris√©', 'apeur√©', 'crainte', 'appr√©hension'],
    'surprise': ['surpris', '√©tonn√©', 'stup√©fait', 'interloqu√©', 'bouche b√©e'],
    'd√©go√ªt': ['d√©go√ªt', 'r√©pulsion', '√©c≈ìur√©', 'aversion', 'r√©vulsion'],
    'neutre': ['normal', 'habituel', 'ordinaire', 'classique', 'standard']
  };

  static getInstance(): WebSpeechSTTService {
    if (!WebSpeechSTTService.instance) {
      WebSpeechSTTService.instance = new WebSpeechSTTService();
    }
    return WebSpeechSTTService.instance;
  }

  constructor() {
    this.initializeSpeechRecognition();
  }

  /**
   * INITIALISATION DU SERVICE DE RECONNAISSANCE VOCALE
   */
  private initializeSpeechRecognition(): void {
    try {
      // V√©rifier support Web Speech API
      const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;
      
      if (!SpeechRecognition) {
        console.warn('‚ö†Ô∏è Web Speech API non support√©e par ce navigateur');
        this.isSupported = false;
        return;
      }

      this.recognition = new SpeechRecognition();
      this.isSupported = true;
      console.log('‚úÖ WebSpeechSTTService initialis√© avec succ√®s');

    } catch (error) {
      console.error('‚ùå Erreur initialisation Web Speech API:', error);
      this.isSupported = false;
    }
  }

  /**
   * D√âMARRAGE SESSION DE RECONNAISSANCE VOCALE
   * Pour utilisation dans sessions th√©rapeutiques conversationnelles
   */
  async startRecognitionSession(
    config: Partial<STTRequest> = {},
    callbacks: {
      onResult: (result: STTResponse) => void;
      onError?: (error: string) => void;
      onEnd?: () => void;
    }
  ): Promise<STTSession | null> {
    if (!this.isSupported || !this.recognition) {
      const error = 'Reconnaissance vocale non support√©e';
      console.error('‚ùå', error);
      callbacks.onError?.(error);
      return null;
    }

    // Arr√™ter session pr√©c√©dente si active
    if (this.currentSession?.isActive) {
      await this.stopRecognitionSession();
      // Attendre que la reconnaissance soit r√©ellement arr√™t√©e
      await new Promise(resolve => setTimeout(resolve, 100));
    }

    try {
      // V√©rifier si une reconnaissance est d√©j√† en cours
      if (this.recognition && this.recognition.abort) {
        this.recognition.abort(); // Force l'arr√™t
        await new Promise(resolve => setTimeout(resolve, 50));
      }

      // Configuration session
      const sessionConfig = { ...this.DEFAULT_CONFIG, ...config };
      const sessionId = `stt_session_${++this.sessionCounter}`;

      // Configuration recognition
      this.recognition.lang = sessionConfig.language;
      this.recognition.continuous = sessionConfig.continuous;
      this.recognition.interimResults = sessionConfig.interimResults;
      this.recognition.maxAlternatives = sessionConfig.maxAlternatives;

      // Cr√©er session
      const session: STTSession = {
        id: sessionId,
        isActive: false,
        startTime: new Date(),
        config: sessionConfig,
        onResult: callbacks.onResult,
        onError: callbacks.onError || (() => {}),
        onEnd: callbacks.onEnd || (() => {})
      };

      // Configuration callbacks recognition
      this.setupRecognitionCallbacks(session);

      // D√©marrer reconnaissance avec v√©rification d'√©tat
      try {
        this.recognition.start();
        session.isActive = true;
        this.currentSession = session;
      } catch (startError) {
        if (startError.name === 'InvalidStateError') {
          // Si d√©j√† d√©marr√©e, forcer l'arr√™t et r√©essayer
          this.recognition.abort();
          await new Promise(resolve => setTimeout(resolve, 100));
          this.recognition.start();
          session.isActive = true;
          this.currentSession = session;
        } else {
          throw startError;
        }
      }

      console.log('üé§ Session reconnaissance vocale d√©marr√©e:', sessionId);
      return session;

    } catch (error) {
      const errorMessage = `Erreur d√©marrage reconnaissance: ${error}`;
      console.error('‚ùå', errorMessage);
      callbacks.onError?.(errorMessage);
      return null;
    }
  }

  /**
   * ARR√äT SESSION DE RECONNAISSANCE VOCALE
   */
  async stopRecognitionSession(): Promise<void> {
    if (!this.currentSession?.isActive || !this.recognition) {
      return;
    }

    try {
      this.recognition.stop();
      this.currentSession.isActive = false;
      
      console.log('üõë Session reconnaissance vocale arr√™t√©e:', this.currentSession.id);
      
      // Callback fin de session
      this.currentSession.onEnd();
      this.currentSession = null;

    } catch (error) {
      console.error('‚ùå Erreur arr√™t reconnaissance:', error);
    }
  }

  /**
   * CONFIGURATION DES CALLBACKS DE RECONNAISSANCE
   * Traitement des r√©sultats et d√©tection √©motionnelle
   */
  private setupRecognitionCallbacks(session: STTSession): void {
    if (!this.recognition) return;

    // R√©sultat reconnaissance
    this.recognition.onresult = (event) => {
      try {
        const speechSession = event.results[event.results.length - 1];
        const transcript = speechSession[0].transcript;
        const confidence = speechSession[0].confidence;
        const isFinal = speechSession.isFinal;

        // Alternatives de transcription
        const alternatives = [];
        for (let i = 0; i < Math.min(speechSession.length, session.config.maxAlternatives); i++) {
          alternatives.push({
            transcript: speechSession[i].transcript,
            confidence: speechSession[i].confidence
          });
        }

        // D√©tection √©motionnelle basique
        const detectedEmotion = this.detectBasicEmotion(transcript);

        // Calculer dur√©e si final
        const speechDuration = isFinal ? 
          (Date.now() - session.startTime.getTime()) / 1000 : 
          undefined;

        const result: STTResponse = {
          transcript,
          confidence,
          isFinal,
          alternatives,
          detectedEmotion,
          speechDuration
        };

        // Callback r√©sultat
        session.onResult(result);

        console.log(`üéôÔ∏è Reconnaissance ${isFinal ? 'finale' : 'partielle'}:`, transcript);
        if (detectedEmotion && detectedEmotion !== 'neutre') {
          console.log(`üòä √âmotion d√©tect√©e: ${detectedEmotion}`);
        }

      } catch (error) {
        console.error('‚ùå Erreur traitement r√©sultat reconnaissance:', error);
        session.onError('Erreur traitement r√©sultat vocal');
      }
    };

    // Erreurs reconnaissance
    this.recognition.onerror = (event) => {
      const errorMessage = this.mapSpeechRecognitionError(event.error);
      console.error('‚ùå Erreur reconnaissance vocale:', errorMessage);
      session.onError(errorMessage);
    };

    // Fin de reconnaissance
    this.recognition.onend = () => {
      if (session.isActive) {
        console.log('üèÅ Reconnaissance vocale termin√©e');
        session.isActive = false;
        session.onEnd();
        
        // Red√©marrer si mode continu et session toujours active
        if (session.config.continuous && this.currentSession?.id === session.id) {
          setTimeout(() => {
            try {
              this.recognition?.start();
              session.isActive = true;
            } catch (error) {
              console.error('‚ùå Erreur red√©marrage reconnaissance continue:', error);
            }
          }, 100);
        }
      }
    };

    // D√©but de reconnaissance
    this.recognition.onstart = () => {
      console.log('üé§ Reconnaissance vocale active');
    };

    // D√©tection de parole
    this.recognition.onspeechstart = () => {
      console.log('üó£Ô∏è Parole d√©tect√©e');
    };

    this.recognition.onspeechend = () => {
      console.log('ü§ê Fin de parole d√©tect√©e');
    };
  }

  /**
   * D√âTECTION √âMOTIONNELLE BASIQUE
   * Analyse simple bas√©e sur mots-cl√©s
   */
  private detectBasicEmotion(transcript: string): string {
    const transcriptLower = transcript.toLowerCase();
    
    // Chercher mots-cl√©s √©motionnels
    for (const [emotion, keywords] of Object.entries(this.EMOTIONAL_KEYWORDS)) {
      const hasKeyword = keywords.some(keyword => 
        transcriptLower.includes(keyword.toLowerCase())
      );
      
      if (hasKeyword) {
        return emotion;
      }
    }

    // D√©tection patterns √©motionnels simples
    if (/[!]{2,}/.test(transcript)) return 'col√®re';
    if (/\?{2,}/.test(transcript)) return 'anxieux';
    if (/\.{3,}/.test(transcript)) return 'triste';

    return 'neutre';
  }

  /**
   * MAPPING ERREURS SPEECH RECOGNITION
   */
  private mapSpeechRecognitionError(error: string): string {
    const errorMap = {
      'no-speech': 'Aucune parole d√©tect√©e. Parlez plus fort ou rapprochez-vous du microphone.',
      'audio-capture': 'Impossible d\'acc√©der au microphone. V√©rifiez les permissions.',
      'not-allowed': 'Acc√®s microphone refus√©. Autorisez l\'acc√®s dans les param√®tres du navigateur.',
      'network': 'Erreur r√©seau. V√©rifiez votre connexion internet.',
      'language-not-supported': 'Langue non support√©e pour la reconnaissance vocale.',
      'service-not-allowed': 'Service de reconnaissance non autoris√©.',
      'bad-grammar': 'Erreur de grammaire dans la reconnaissance.',
      'aborted': 'Reconnaissance vocale interrompue.'
    };

    return errorMap[error as keyof typeof errorMap] || `Erreur inconnue: ${error}`;
  }

  /**
   * UTILITAIRES PUBLICS
   */

  // V√©rifier support reconnaissance vocale
  isRecognitionSupported(): boolean {
    return this.isSupported;
  }

  // √âtat session actuelle
  getCurrentSession(): STTSession | null {
    return this.currentSession;
  }

  // Tester reconnaissance vocale
  async testRecognition(): Promise<boolean> {
    if (!this.isSupported) return false;

    return new Promise((resolve) => {
      let testCompleted = false;

      const testSession = this.startRecognitionSession({
        continuous: false,
        interimResults: false
      }, {
        onResult: (result) => {
          if (!testCompleted) {
            testCompleted = true;
            this.stopRecognitionSession();
            resolve(true);
          }
        },
        onError: () => {
          if (!testCompleted) {
            testCompleted = true;
            resolve(false);
          }
        }
      });

      // Timeout test apr√®s 3 secondes
      setTimeout(() => {
        if (!testCompleted) {
          testCompleted = true;
          this.stopRecognitionSession();
          resolve(false);
        }
      }, 3000);
    });
  }

  /**
   * M√âTHODES SP√âCIALIS√âES TH√âRAPIE
   */

  // Reconnaissance adapt√©e expert th√©rapeutique
  async startTherapeuticRecognition(
    expertId: string,
    emotionalContext: string,
    callbacks: {
      onResult: (result: STTResponse) => void;
      onError?: (error: string) => void;
      onEnd?: () => void;
    }
  ): Promise<STTSession | null> {
    // Configuration optimis√©e selon expert
    const config: Partial<STTRequest> = {
      language: 'fr-FR',
      continuous: true,
      interimResults: true,
      maxAlternatives: 2,
      expertId,
      emotionalContext
    };

    // Ajustements selon expert
    if (expertId === 'dr_alex_mindfulness') {
      config.continuous = false; // Plus adapt√© pour m√©ditation guid√©e
    } else if (expertId === 'dr_aicha_culturelle') {
      // Pourrait supporter arabe dans le futur
      config.language = 'fr-FR'; // Pour l'instant fran√ßais seulement
    }

    return await this.startRecognitionSession(config, callbacks);
  }

  // Analyse sentiments pour session th√©rapeutique
  analyzeTherapeuticSentiment(transcript: string): {
    primaryEmotion: string;
    emotionalIntensity: number;
    therapeuticIndicators: string[];
    needsAttention: boolean;
  } {
    const detectedEmotion = this.detectBasicEmotion(transcript);
    
    // Indicateurs th√©rapeutiques
    const therapeuticIndicators = [];
    let needsAttention = false;
    
    // Mots indicateurs de d√©tresse
    const distressKeywords = ['suicide', 'mourir', 'dispara√Ætre', 'inutile', 'd√©sespoir'];
    const hasDistressKeywords = distressKeywords.some(keyword => 
      transcript.toLowerCase().includes(keyword)
    );
    
    if (hasDistressKeywords) {
      therapeuticIndicators.push('Indicateurs de d√©tresse d√©tect√©s');
      needsAttention = true;
    }

    // Intensit√© √©motionnelle bas√©e sur ponctuation et r√©p√©titions
    let emotionalIntensity = 5; // Base neutre
    
    if (/[!]{1,}/.test(transcript)) emotionalIntensity += 2;
    if (/[?]{1,}/.test(transcript)) emotionalIntensity += 1;
    if (/(.)\1{2,}/.test(transcript)) emotionalIntensity += 1; // Lettres r√©p√©t√©es
    if (transcript.length > 100) emotionalIntensity += 1; // Messages longs = engagement
    
    emotionalIntensity = Math.min(10, Math.max(1, emotionalIntensity));

    // Indicateurs d'engagement positif
    const engagementKeywords = ['comprendre', 'apprendre', 'progresser', 'merci', 'aide'];
    const hasEngagementKeywords = engagementKeywords.some(keyword => 
      transcript.toLowerCase().includes(keyword)
    );
    
    if (hasEngagementKeywords) {
      therapeuticIndicators.push('Engagement th√©rapeutique positif');
    }

    return {
      primaryEmotion: detectedEmotion,
      emotionalIntensity,
      therapeuticIndicators,
      needsAttention
    };
  }

  /**
   * NETTOYAGE ET DIAGNOSTIC
   */

  // Statistiques utilisation
  getUsageStats(): {
    isSupported: boolean;
    currentSessionActive: boolean;
    totalSessions: number;
  } {
    return {
      isSupported: this.isSupported,
      currentSessionActive: this.currentSession?.isActive || false,
      totalSessions: this.sessionCounter
    };
  }

  // Nettoyage ressources
  cleanup(): void {
    this.stopRecognitionSession();
    if (this.recognition) {
      this.recognition = null;
    }
    console.log('üßπ WebSpeechSTTService nettoy√©');
  }
}

// Export instance singleton
export const webSpeechSTTService = WebSpeechSTTService.getInstance();

// Types pour int√©gration globale
declare global {
  interface Window {
    SpeechRecognition: any;
    webkitSpeechRecognition: any;
  }
}

export default WebSpeechSTTService;